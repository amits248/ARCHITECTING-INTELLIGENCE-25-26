# -*- coding: utf-8 -*-
"""ASSIGNMENT 2 (240264)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14wVWlnHHmmQDECCcPU_Pvcpgrxzv7mxq

PyTorch or NumPy ***function***
"""

import torch
import math

def attention(Q, K, V):

    d_k = Q.size(-1)

    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    weights = torch.softmax(scores, dim=-1)
    output = torch.matmul(weights, V)

    return output, weights

"""QUESTION 4

"""

import torch
import math

def masked_attention(Q, K, V):
    n, d_k = Q.shape

    scores = torch.matmul(Q, K.T) / math.sqrt(d_k)
    mask = torch.triu(torch.ones(n, n), diagonal=1)
    scores = scores.masked_fill(mask == 1, float('-inf'))

    weights = torch.softmax(scores, dim=-1)
    output = torch.matmul(weights, V)

    return output, weights

torch.manual_seed(0)

Q = torch.randn(4, 8)
K = torch.randn(4, 8)
V = torch.randn(4, 8)

_, weights = masked_attention(Q, K, V)
print(weights)

"""QUESTION 5"""

!pip install transformers torch matplotlib seaborn

import torch
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained(
    "bert-base-uncased",
    output_attentions=True
)

model.eval()

sentence = "The cat sat on the mat"

inputs = tokenizer(sentence, return_tensors="pt")
outputs = model(**inputs)

attentions = outputs.attentions
tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

layer = 0      # first layer
head = 0       # first attention head

attn = attentions[layer][0, head].detach().numpy()

plt.figure(figsize=(8, 6))
sns.heatmap(attn, xticklabels=tokens, yticklabels=tokens, cmap="viridis")
plt.title(f"Layer {layer+1}, Head {head+1}")
plt.xlabel("Key")
plt.ylabel("Query")
plt.show()

fig, axes = plt.subplots(2, 3, figsize=(15, 8))
heads = [0, 1, 2, 3, 4, 5]

for ax, h in zip(axes.flat, heads):
    attn = attentions[0][0, h].detach().numpy()
    sns.heatmap(attn, xticklabels=tokens, yticklabels=tokens,
                cmap="viridis", ax=ax, cbar=False)
    ax.set_title(f"Head {h+1}")

plt.tight_layout()
plt.show()